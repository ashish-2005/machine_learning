{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e58a0cc",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0e69a",
   "metadata": {},
   "source": [
    "### Normal Equation \n",
    "* This equation used to find weights of different features<br>\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-1-10.png'><br>\n",
    "* θ: hypothesis parameters that define it the best. \n",
    "<br>X: Input feature value of each instance. \n",
    "<br>Y: Output value of each instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1645cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.517459Z",
     "iopub.status.busy": "2022-10-13T06:52:52.517026Z",
     "iopub.status.idle": "2022-10-13T06:52:52.523677Z",
     "shell.execute_reply": "2022-10-13T06:52:52.522554Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.517408Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (787721509.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [5], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    y = (3 * x ) + 4 + np.random.rand(100,1))\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "#genrating random linear data\n",
    "x = np.random.rand(100,1)\n",
    "y = (3 * x ) + 4 + np.random.rand(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e206a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.526291Z",
     "iopub.status.busy": "2022-10-13T06:52:52.525827Z",
     "iopub.status.idle": "2022-10-13T06:52:52.732844Z",
     "shell.execute_reply": "2022-10-13T06:52:52.731775Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.526258Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(x,y,'b.')\n",
    "plt.xlabel('X₁',fontsize=15)\n",
    "plt.ylabel('y',fontsize=15)\n",
    "plt.title('Linear Data',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cf727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.735037Z",
     "iopub.status.busy": "2022-10-13T06:52:52.734197Z",
     "iopub.status.idle": "2022-10-13T06:52:52.743483Z",
     "shell.execute_reply": "2022-10-13T06:52:52.742318Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.735002Z"
    }
   },
   "outputs": [],
   "source": [
    "#finding transpose , inverse of X matrix\n",
    "X = np.c_[np.ones((100,1)),x] #add x0=1 to each instance\n",
    "X_trans = X.T\n",
    "X_inv = np.linalg.inv(X_trans.dot(X))\n",
    "theta_best = X_inv.dot(X_trans).dot(y) # Normal Equation\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87175320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.746214Z",
     "iopub.status.busy": "2022-10-13T06:52:52.745878Z",
     "iopub.status.idle": "2022-10-13T06:52:52.759159Z",
     "shell.execute_reply": "2022-10-13T06:52:52.757687Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.746184Z"
    }
   },
   "outputs": [],
   "source": [
    "# predicting with new data\n",
    "x_new = np.array([[0],[0.99]])\n",
    "X_new = np.c_[(np.ones((2,1)),x_new)]\n",
    "y_predicted = X_new.dot(theta_best)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8b2d46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.760945Z",
     "iopub.status.busy": "2022-10-13T06:52:52.760579Z",
     "iopub.status.idle": "2022-10-13T06:52:52.911014Z",
     "shell.execute_reply": "2022-10-13T06:52:52.909724Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.760914Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualising results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x_new,y_predicted,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr-\u001b[39m\u001b[38;5;124m'\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction line\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x,y,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb.\u001b[39m\u001b[38;5;124m'\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_new' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualising results\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(x_new,y_predicted,'r-',label='prediction line')\n",
    "plt.plot(x,y,'b.',label='train/data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077a492",
   "metadata": {},
   "source": [
    "* We can do same thing with scikit-learn's linear_regressor model and it perform better then the equation as it uses SVD (Singular value Decomposition) to find the pseudoinverse of matrix\n",
    "* another benefit is that in Normal eq we had to find inverse of it which is not possible in singular matrix but pseudoinverse is defined for every matrix<br>\n",
    "<b>Note : Study SVD from Notebook</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3663dd",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "* Its is a optimizing algorithim capable to find optimal solution to a wide range of problems\n",
    "* The main idea is to tweak parameter iteratively to minimize a cost function<br>\n",
    "<img src = 'https://149695847.v2.pressablecdn.com/wp-content/uploads/2022/07/image-99.png' height = '200'  width = '200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641db1b",
   "metadata": {},
   "source": [
    "### Batch Gradient Decent\n",
    "* we find the partial derevative of the loss function with all the parameter then minimize the loss (read the gradiant decent step )\n",
    "* In this we calculate over whole data set which turn out to be a long process with big datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366d612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.912271Z",
     "iopub.status.busy": "2022-10-13T06:52:52.911946Z",
     "iopub.status.idle": "2022-10-13T06:52:52.929084Z",
     "shell.execute_reply": "2022-10-13T06:52:52.927906Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.912241Z"
    }
   },
   "outputs": [],
   "source": [
    "#performing gradient decent\n",
    "learning_rate = 0.1\n",
    "n_itter = 1000\n",
    "m = len(X)\n",
    "\n",
    "theta = np.random.randn(2,1)  #creating a random initial theta\n",
    "\n",
    "for i in range(n_itter):\n",
    "    theta_new = (2/m) * ( X.T.dot( X.dot(theta) - y ) ) # this formula is the derivative of MSE with respect to theta\n",
    "    theta = theta - (theta_new * learning_rate)\n",
    "theta_gd = theta\n",
    "theta_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3265412",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Decent\n",
    "* The formula is quite similar to gradient decent but it differ in learning rate selection and data used\n",
    "* By convention we use single instance and determine learning rate at each at each itteration called <i>learning_schedule</i>\n",
    "* by convention we iterate <i>m</i> (ie. length of X) times in each round called epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25eaca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.930461Z",
     "iopub.status.busy": "2022-10-13T06:52:52.930102Z",
     "iopub.status.idle": "2022-10-13T06:52:52.993874Z",
     "shell.execute_reply": "2022-10-13T06:52:52.992779Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.930422Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing SGD \n",
    "n_epoch = 50\n",
    "t0 , t1 = 5 ,50 #learning schedule hyperparameter\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t0 + t)\n",
    "\n",
    "for epoch in range (n_epoch):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X[random_index : random_index+1] #select random instance\n",
    "        yi = y[random_index : random_index+1]\n",
    "        theta_new = (2/m) * ( xi.T.dot( xi.dot(theta) - yi ) )\n",
    "        learning_rate = learning_schedule(epoch * m + 1)\n",
    "        theta = theta -(theta_new * learning_rate)\n",
    "theta_SGD = theta\n",
    "theta_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981ea63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:52.996095Z",
     "iopub.status.busy": "2022-10-13T06:52:52.995633Z",
     "iopub.status.idle": "2022-10-13T06:52:53.007153Z",
     "shell.execute_reply": "2022-10-13T06:52:53.006228Z",
     "shell.execute_reply.started": "2022-10-13T06:52:52.996051Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can do same with scikit learn using SGDRegressor in linear_model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1000,tol=1e-3,eta0=0.1)\n",
    "sgd.fit(x,y.ravel())\n",
    "\n",
    "sgd.intercept_,sgd.coef_\n",
    "#see we are getting very much same results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5cd737",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "<img src='https://i.ibb.co/m5GrF0k/conclusion.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e946497",
   "metadata": {},
   "source": [
    "# Polynomial Regrssion\n",
    "* We can use linear model to fit non-linear data by adding power to each feature this technique is called polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43bec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:53.012235Z",
     "iopub.status.busy": "2022-10-13T06:52:53.011899Z",
     "iopub.status.idle": "2022-10-13T06:52:53.202063Z",
     "shell.execute_reply": "2022-10-13T06:52:53.200858Z",
     "shell.execute_reply.started": "2022-10-13T06:52:53.012204Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating non linear data ie. quadritic in this case\n",
    "x = 6.5* np.random.rand(100,1) - 4\n",
    "y = (0.5*(x**2)) + 2*(x) + 2.5  +np.random.randn(100,1)  # aX² + bX + c\n",
    "\n",
    "plt.figure(figsize=(6,3.5))\n",
    "plt.plot(x,y,'b.')\n",
    "plt.show()\n",
    "\n",
    "#look very much like a quadritic's graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ea461",
   "metadata": {},
   "source": [
    "* Clearly a straight line will never fit data so we will transform our feature from linear to 2nd degree polynomial with sklearn's PolynomialFeatures class \n",
    "* then we can fit it with a linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fe873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:53.203873Z",
     "iopub.status.busy": "2022-10-13T06:52:53.203365Z",
     "iopub.status.idle": "2022-10-13T06:52:53.413918Z",
     "shell.execute_reply": "2022-10-13T06:52:53.412638Z",
     "shell.execute_reply.started": "2022-10-13T06:52:53.203825Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_feature = PolynomialFeatures(degree=2,include_bias=False)\n",
    "x_poly = poly_feature.fit_transform(x)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_poly,y)\n",
    "\n",
    "intercept = lin_reg.intercept_ # this will give the constant of eq.\n",
    "coff      = lin_reg.coef_ # this will give the coff of X , X² respectivly \n",
    "\n",
    "#plotting results\n",
    "x_pred = [-3.25, -3.0, -2.75, -2.5, -2.25, -2.0, -1.75, -1.5, -1.25, -1.0, -0.75, -0.5, -0.25,\n",
    "          0.0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75]\n",
    "y_pred = [(coff[0][1] * (i**2) + coff[0][0]*i + intercept) for i in x_pred]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(x,y,'b.',label='data')\n",
    "plt.plot(x_pred,y_pred,'r-',label='prediction')\n",
    "plt.xlim(-3.5,2.5)\n",
    "plt.ylim(-2,10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd99ec",
   "metadata": {},
   "source": [
    "* we can make any degree of feature but a big number might end up overfitting on the other hand a smaller one will underfit hence a optimal number is desired as in above we know the data is gerated with a quad. eq. so we used a 2nd deg. data which fit it goodly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860925e8",
   "metadata": {},
   "source": [
    "# Leraning Curves\n",
    "* These are the plot of performance of model on training and validation set as the function of training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a1270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:53.415660Z",
     "iopub.status.busy": "2022-10-13T06:52:53.415290Z",
     "iopub.status.idle": "2022-10-13T06:52:53.425292Z",
     "shell.execute_reply": "2022-10-13T06:52:53.424476Z",
     "shell.execute_reply.started": "2022-10-13T06:52:53.415624Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#this function will plot the learning curve of model\n",
    "def plot_learning_curve(model,x,y):\n",
    "    x_train , x_val , y_train , y_val = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "    train_error , val_error = [] , []\n",
    "    for m in range(1,len(x_train)): #this loop will find train and val error at different train size\n",
    "        model.fit(x_train[:m],y_train[:m])\n",
    "        \n",
    "        y_train_predict = model.predict(x_train[:m])\n",
    "        y_val_predict = model.predict(x_val)\n",
    "        \n",
    "        train_err = mean_squared_error(y_train[:m] , y_train_predict)\n",
    "        val_err = mean_squared_error(y_val , y_val_predict)\n",
    "        \n",
    "        train_error.append(train_err)\n",
    "        val_error.append(val_err)\n",
    "    \n",
    "    #plotting result\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(np.sqrt(train_error),'b-',label='Train Error')\n",
    "    plt.plot(np.sqrt(val_error),'r-',label='Validation Error')\n",
    "    plt.xlabel('Train_set_size')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fb3a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:53.427239Z",
     "iopub.status.busy": "2022-10-13T06:52:53.426507Z",
     "iopub.status.idle": "2022-10-13T06:52:53.723233Z",
     "shell.execute_reply": "2022-10-13T06:52:53.721928Z",
     "shell.execute_reply.started": "2022-10-13T06:52:53.427205Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "plot_learning_curve(lin_model,x,y) # x and y are from polynomail regressor model\n",
    "#clearly the model is underfitting as we are using a linear model over a quadritic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f397aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:53.724759Z",
     "iopub.status.busy": "2022-10-13T06:52:53.724427Z",
     "iopub.status.idle": "2022-10-13T06:52:54.066396Z",
     "shell.execute_reply": "2022-10-13T06:52:54.065336Z",
     "shell.execute_reply.started": "2022-10-13T06:52:53.724731Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "poly_regressor = Pipeline([\n",
    "    ('polynomial_feature',PolynomialFeatures(degree=2,include_bias=False)),\n",
    "    ('lin_reg',LinearRegression())\n",
    "])\n",
    "\n",
    "plot_learning_curve(poly_regressor,x,y)\n",
    "#now RMSE is quite low and get constant after certain training-set-size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a50ebe",
   "metadata": {},
   "source": [
    "# Regularizing Linear Models\n",
    "* To reduce overfitting we have to regularize the model ie. to constrain it\n",
    "* to regularize linear model we use Ridge regressor , lasso regressor and elastic net to contrain the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be771fb5",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "* Its the regualrized version of linear regression with an regularization term added to cost function<br>\n",
    "<img src='https://i.ibb.co/d0MtJ3G/ridge-reg.png'><br>\n",
    "* It not only fit the data but also keep model weight as small as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5286b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:54.068283Z",
     "iopub.status.busy": "2022-10-13T06:52:54.067862Z",
     "iopub.status.idle": "2022-10-13T06:52:54.079381Z",
     "shell.execute_reply": "2022-10-13T06:52:54.078123Z",
     "shell.execute_reply.started": "2022-10-13T06:52:54.068239Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "ridge_reg = Ridge(alpha=1,solver='cholesky')\n",
    "ridge_reg.fit(x,y)\n",
    "\n",
    "sgd_reg = SGDRegressor(penalty='l2')\n",
    "# penalty='l2' indicates to add regularization term to cost function equal to half of square of weight (ie. simply ridge regression) \n",
    "sgd_reg.fit(x,y)\n",
    "\n",
    "print('Ridge Regression Prediction',ridge_reg.predict([[1.5]]))\n",
    "print('Stochastic Gradient Decent Prediction',sgd_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbbee5",
   "metadata": {},
   "source": [
    "### LASSO Regression\n",
    "* LASSO is short form for <b>least absolute shrinkage and selection opration</b> it uses l1 norm unlike l2 norm used in ridge regression<br>\n",
    "<img src='https://i.ibb.co/4KXqLGC/lasso-reg.png'><br>\n",
    "* it also a type of regularized linear regression but what makes it different is that it completely eliminates the weight of least important feature (ie set them to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3598df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:54.081274Z",
     "iopub.status.busy": "2022-10-13T06:52:54.080891Z",
     "iopub.status.idle": "2022-10-13T06:52:54.096871Z",
     "shell.execute_reply": "2022-10-13T06:52:54.095649Z",
     "shell.execute_reply.started": "2022-10-13T06:52:54.081240Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.1) \n",
    "lasso.fit(x,y)\n",
    "lasso.predict([[1.5]])\n",
    "#we will find similar result with SGDRegressor with penalty = l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066defc9",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "* it is the middleground between ridge and lasso regression \n",
    "* Its regularization term is simple mix of both ridge and lasso and can be controlled by r\n",
    "* At r = 0 its equivalent to ridge and at r = 1 lasso<br>\n",
    "<img src='https://i.ibb.co/jMhZ5Rz/elastic-net.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af846649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:54.098574Z",
     "iopub.status.busy": "2022-10-13T06:52:54.098224Z",
     "iopub.status.idle": "2022-10-13T06:52:54.111671Z",
     "shell.execute_reply": "2022-10-13T06:52:54.110485Z",
     "shell.execute_reply.started": "2022-10-13T06:52:54.098544Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1,l1_ratio=0.5) # li_ratio corresponds to r\n",
    "elastic_net.fit(x,y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0393a5",
   "metadata": {},
   "source": [
    "### Early Stopping \n",
    "* A very unique way to regularize iterative learning algorithims such as Gradient Decent is to stop training as soon as error reaches minimum \n",
    "* Its dificult to tell the error reached minimum or not for SGD and mini-batch GD so we can stop when error has been above minimum for sometime "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad025337",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "* It is commonly used to estimate the probability of instances belonging to a perticular class (generally used for binary classification)<br>\n",
    "<div><img src='https://i.ibb.co/N9pJ4Jn/prob.png' style=\"width:25%\"></div><div><img src='https://i.ibb.co/M1d482D/sigmoid.png' style=\"width:25%\"></div><br>\n",
    "* if p̂ >= 0.50 then ŷ = 1 or if p̂ < 0.50 then ŷ = 0\n",
    "* Logistic Regression Loss function (log-loss)<br>\n",
    "<img src='https://i.ibb.co/Xyjjm41/log-loss.png' style='width:50%'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c731850",
   "metadata": {},
   "source": [
    "#### ilustration with iris(flower) dataset\n",
    "this model will classify wether the flower is iris-virginica or not on the basis of petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951437a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:54.114330Z",
     "iopub.status.busy": "2022-10-13T06:52:54.113390Z",
     "iopub.status.idle": "2022-10-13T06:52:54.124378Z",
     "shell.execute_reply": "2022-10-13T06:52:54.123050Z",
     "shell.execute_reply.started": "2022-10-13T06:52:54.114296Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "x = iris['data'][:,3:] #loading only flowers petal width\n",
    "y = (iris['target'] == 2).astype(np.int64) # 1 if iris-virginica , else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cdeab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:54.126814Z",
     "iopub.status.busy": "2022-10-13T06:52:54.126384Z",
     "iopub.status.idle": "2022-10-13T06:52:54.141125Z",
     "shell.execute_reply": "2022-10-13T06:52:54.140204Z",
     "shell.execute_reply.started": "2022-10-13T06:52:54.126780Z"
    }
   },
   "outputs": [],
   "source": [
    "#training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg_reg = LogisticRegression()\n",
    "lg_reg.fit(x,y)\n",
    "print('trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffe17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:54.143420Z",
     "iopub.status.busy": "2022-10-13T06:52:54.142902Z",
     "iopub.status.idle": "2022-10-13T06:52:54.371977Z",
     "shell.execute_reply": "2022-10-13T06:52:54.370666Z",
     "shell.execute_reply.started": "2022-10-13T06:52:54.143376Z"
    }
   },
   "outputs": [],
   "source": [
    "#predicting\n",
    "x_new = np.linspace(0,3,1000).reshape(-1,1)\n",
    "y_prob = lg_reg.predict_proba(x_new)\n",
    "\n",
    "#plotting result\n",
    "plt.figure(figsize=(6,2.5))\n",
    "plt.plot(x_new,y_prob[:,1],'g-',label='Iris-Virginica')\n",
    "plt.plot(x_new,y_prob[:,0],'b--',label='Not-Iris-Virginica')\n",
    "plt.ylabel('Probability',fontsize=12)\n",
    "plt.xlabel('Petal_Width',fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96583827",
   "metadata": {},
   "source": [
    "### SoftMax Regression { Multinomial Logistic Regression}\n",
    "* When a instance(x) is given softmax regression compute a score{sₖ(x)} for each class then estimate the probability of x lying in class by softmax function.\n",
    "* Softmax score is equivalent to the Linear regression prediction for x but with theta(θ) different for different classes<br>\n",
    "<img src='https://i.ibb.co/rp5vxH4/softmax-score.png' width='25%'><br>\n",
    "* Softmax function (also known as <i>normalized exponential</i>) is the ratio of the exp of that score and sum of exp of all score (see image)<br>\n",
    "<img src='https://i.ibb.co/WnN2LRQ/softmax-function.png' width='32%'><br>\n",
    "* And at last just like Logistic Regression Softmax too predict the class with the highest estimated probability or highest score<br>\n",
    "<img src='https://i.ibb.co/7GkMQYs/softmax-prediction.png' width='55%'><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed99137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T06:52:54.373977Z",
     "iopub.status.busy": "2022-10-13T06:52:54.373531Z",
     "iopub.status.idle": "2022-10-13T06:52:54.401077Z",
     "shell.execute_reply": "2022-10-13T06:52:54.400153Z",
     "shell.execute_reply.started": "2022-10-13T06:52:54.373933Z"
    }
   },
   "outputs": [],
   "source": [
    "x = iris['data'][:,(2,3)] # petals width and length\n",
    "y = iris['target']\n",
    "\n",
    "#this will change Logistic to softmax regression with solver lbfgs and regulerization(c=10)\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial',solver='lbfgs',C=10)\n",
    "softmax_reg.fit(x,y)\n",
    "\n",
    "print(softmax_reg.predict([[5,2]]),softmax_reg.predict_proba([[5,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85bc8c",
   "metadata": {},
   "source": [
    "# End\n",
    "If you made it to the end , an upvote will be appritiated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
