{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6071bb",
   "metadata": {},
   "source": [
    "### ***Ensemble learning exercise***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea9da6",
   "metadata": {},
   "source": [
    "##### 1) If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "***ANS :-*** Yes we can achieve more than 95% accuracy even though each model have only accuracy of 95% itself,this is because some models are guessing some instance right and some other models are guessing other right. This logic will be applicable till models are different , ie. either algorithim is defferent or train_data they are trained on are different <br>\n",
    "Since , each model have accuracy of 95% we cannot reduce importance/weight of any model so AdaBoosting is not an option hence we can use hard-voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44788b5",
   "metadata": {},
   "source": [
    "##### 2)What is the difference between hard and soft voting classifiers?\n",
    "***ANS :-*** Using the majority predicted class as final prediction is called **Hard-Voting**<br>\n",
    "Whereas using class with highest averaged probability as final prediction is called **Soft-Voting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb848f68",
   "metadata": {},
   "source": [
    "##### 3) Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "***ANS :-*** It is possible to speed up training of bagging, pasting, and random forests because each predictor is independent of the others.<br>\n",
    "boosting ensemble predictors are built based on the previous predictor, so you separating training will not help.<br>\n",
    "For stacking ensembles, all predictors for one layer has to be trained before the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919afa0e",
   "metadata": {},
   "source": [
    "##### 4) What is the benefit of out-of-bag evaluation?\n",
    "***ANS :-*** Out-of-bag data are the training instances that left because of random selection in bagging and pasting.<br>\n",
    "Its major benifit is that , you need no extra data for validation these unseen instances can be used as validation-set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8f987",
   "metadata": {},
   "source": [
    "##### 5) What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
    "***ANS :-*** Extra-Trees get their randomness from the fact that , insted of finding best-threshold for feature like DecisionTree(base_estimator for RandomForest) it take random threshold.<br>\n",
    "This extra Randomness trade a *little bias for better varience*<br>\n",
    "Extra Trees is much *faster*. This is because instead of looking for the optimal split at each node it does it randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd186cc7",
   "metadata": {},
   "source": [
    "##### 6) your AdaBoost ensemble underfits the training data,what hyperparameters you tweak and how?\n",
    "***ANS :-*** We can increase the number of estimators(*n_estimators*) , we can reduce the regulerization on *base_estimator* , and we can also increse the *learning_rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69dffa",
   "metadata": {},
   "source": [
    "##### 7) If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "***ANS :-*** We can use early stopping(because more estimator lead to overfitting) , reducing n_estimators , reducing learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07693074",
   "metadata": {},
   "source": [
    "##### 8) Load the MNIST data , and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89af2ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T09:51:41.135213Z",
     "iopub.status.busy": "2022-11-27T09:51:41.134771Z",
     "iopub.status.idle": "2022-11-27T09:51:41.209006Z",
     "shell.execute_reply": "2022-11-27T09:51:41.207763Z",
     "shell.execute_reply.started": "2022-11-27T09:51:41.135173Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "mnist = loadmat(\"/kaggle/input/mnist-mat/mnist-original.mat\")\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce01687f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T09:51:41.211920Z",
     "iopub.status.busy": "2022-11-27T09:51:41.211529Z",
     "iopub.status.idle": "2022-11-27T09:51:41.265600Z",
     "shell.execute_reply": "2022-11-27T09:51:41.264441Z",
     "shell.execute_reply.started": "2022-11-27T09:51:41.211885Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X , X_test , y , y_test = train_test_split(mnist_data,mnist_label,\n",
    "                                                 test_size=0.14285714285,random_state=42)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.16666666666,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55b85036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T09:51:41.267485Z",
     "iopub.status.busy": "2022-11-27T09:51:41.267075Z",
     "iopub.status.idle": "2022-11-27T10:12:44.057399Z",
     "shell.execute_reply": "2022-11-27T10:12:44.056180Z",
     "shell.execute_reply.started": "2022-11-27T09:51:41.267451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest Score  : 96.96000000000001%\n",
      "SupportVector Score : 97.94%\n",
      "ExtraTree Score     : 97.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Models training ####\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "RFC = RandomForestClassifier(n_jobs=-1)\n",
    "SVC = SVC(probability=True)\n",
    "ETC = ExtraTreesClassifier(n_jobs=-1)\n",
    "\n",
    "RFC.fit(X_train,y_train)\n",
    "SVC.fit(X_train,y_train)\n",
    "ETC.fit(X_train,y_train)\n",
    "\n",
    "#### Predictions ####\n",
    "rfc_pred = RFC.predict(X_val)\n",
    "svc_pred = SVC.predict(X_val)\n",
    "etc_pred = ETC.predict(X_val)\n",
    "\n",
    "#### Scores ####\n",
    "print(f'''\n",
    "RandomForest Score  : {accuracy_score(y_val,rfc_pred)*100}%\n",
    "SupportVector Score : {accuracy_score(y_val,svc_pred)*100}%\n",
    "ExtraTree Score     : {accuracy_score(y_val,etc_pred)*100}%\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4275f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T10:12:44.059535Z",
     "iopub.status.busy": "2022-11-27T10:12:44.059184Z",
     "iopub.status.idle": "2022-11-27T10:33:50.692521Z",
     "shell.execute_reply": "2022-11-27T10:33:50.691048Z",
     "shell.execute_reply.started": "2022-11-27T10:12:44.059502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier accuracy : 98.00999999999999%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rfc',RFC),('svc',SVC),('etc',ETC)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train,y_train)\n",
    "print(f\"Voting Classifier accuracy : {accuracy_score(y_val,voting_clf.predict(X_val))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359317bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T10:46:47.676418Z",
     "iopub.status.busy": "2022-11-27T10:46:47.675971Z",
     "iopub.status.idle": "2022-11-27T10:50:14.628893Z",
     "shell.execute_reply": "2022-11-27T10:50:14.627746Z",
     "shell.execute_reply.started": "2022-11-27T10:46:47.676385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST DATASET RESULTS\n",
      "\n",
      "RandomForest Score  : 96.78%\n",
      "SupportVector Score : 97.95%\n",
      "ExtraTree Score     : 97.00999999999999%\n",
      "Voting Classifier accuracy : 97.99%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_pred = RFC.predict(X_test)\n",
    "svc_pred = SVC.predict(X_test)\n",
    "etc_pred = ETC.predict(X_test)\n",
    "\n",
    "print(f'''\n",
    "TEST DATASET RESULTS\n",
    "\n",
    "RandomForest Score  : {accuracy_score(y_test,rfc_pred)*100}%\n",
    "SupportVector Score : {accuracy_score(y_test,svc_pred)*100}%\n",
    "ExtraTree Score     : {accuracy_score(y_test,etc_pred)*100}%\n",
    "Voting Classifier accuracy : {accuracy_score(y_test,voting_clf.predict(X_test))*100}%\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf9d00",
   "metadata": {},
   "source": [
    "##### 9) Creat a stacking model with the help of previous classifier and data\n",
    "all we need to do is to take another classifier which will act like a blender , and train it with X ie. predictions of other classifier and y as target class , thats it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f37da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
