{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6420daca",
   "metadata": {},
   "source": [
    "## ***Dimensionality Reduction***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d254e",
   "metadata": {},
   "source": [
    "#### The Curse of Dimensionality\n",
    "* It is a fact that things change **Drastically** with increase in dimensions. For example , the chances of a random point to lie within 0.001 of a border in 2D is 0.4% which increase to 99.9999% in 10,000 dimension\n",
    "* In theory the cure for curse of dimensionality is to increase the number of training instances to reach density , which is not very practical because for a data with just 100 feature will require equal number of instances as the number of atoms in observable universe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63435ea",
   "metadata": {},
   "source": [
    "#### The Main Approches for Dimensionality Reduction\n",
    "***\n",
    "##### **1) Projection**\n",
    "* In most real world problem, training instances are *not* spread uniformly. Many Features are almost constant where as some are highly correlated. This tell us that the data can lie on much lower-dimension subspace of the high-dimensional space.\n",
    "* The following is an example,see how training instances lie close to the plane. So to reduce dimension(3D-2D) we can take the projection of instances on the plane which will give us a 2D dataset\n",
    "<br>\n",
    "<img src='https://i.ibb.co/t4WwnB3/1.png' width=40% align=left><img src='https://i.ibb.co/MhrMrFg/2.png' width=30% align=middle><br>\n",
    "* However , its not the best way to do dimensionality reduction as many(most) dataset have twist and turns for example Swis roll (dummy dataset) there is simply no way reduce dimension with projection as will create a more messy data than before<br>\n",
    "<img src='https://i.ibb.co/sC88LQ0/3.png' width=35%><br>\n",
    "\n",
    "##### 2) **Manifold Learning**\n",
    "* The above mentioned Swiss roll data is an example of 2D manifold . That means it is 2D shape that is bent or rolled to a heigher dimension space(2D is rolled to 3D in Swiss roll). More generally a d-dimensional mainfold is a part of n-dimensional space and d < n.\n",
    "* Many Dimensionality reduction work on modeling the manifold on which training instance lie called manifold learning. These algorithm relies on some manifold-assumptions called **manifold hypothesis** , which holds that most real world high-dimensional dataset lie closer to much lower-dimensional manifolds, another assumption is that our task(regression or classification) will be simpler if expresed in lower-dimensions\n",
    "<img src='https://i.ibb.co/r5kGmsm/4.png' width=40%><br>\n",
    "* we can see in above image that in first case our task(classification) become simpler after manifold reduction , where as it become complicated in second case (it was much simpler to classify in 3D with a x1=5 plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c58adc2",
   "metadata": {},
   "source": [
    "#### ***PCA*** (Principal Component Analysis)\n",
    "The simple idea behind PCA is to find an optimal hyperplane and project the instances on it.\n",
    "<br><br>***Finding right hyperplane to preserve maximum varience***\n",
    "***\n",
    "* Its resonable to select hyperplane which preserve maximum varience as it will most likely lose less information another way to justify , to find hyperplane which minimises the mean squared distance between the original point and projection<br><br>\n",
    "\n",
    "***Principal Components Analysis***\n",
    "***\n",
    "* PCA identifies the axis that accounts largest ammount of varience in the training dataset. Number of these can be made on a n-dimensional space is n . and each axis is orthagonal to all the previous axis. \n",
    "* The unit vector of the iᵗʰ axis is called iᵗʰ *principal component* \n",
    "* We can find these *principal components* with help of *Singular Value Decomposition(SVD)* , which convert training set **X** into matrix multiplication of three matrix $UΣV^{T}$ where **V** contains all the principal components\n",
    "* we can find PC with help of NumPy's scd() function which return 3 matrixs U,Σ,Vᵀ respectivly<br>\n",
    "```python\n",
    "import numpy as np\n",
    "X_centered = X - X.mean(axis=0) #X is a 3D dataset\n",
    "U,s,Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:,0]\n",
    "c2 = Vt.T[:,1]\n",
    "```\n",
    "PCA assumes that the dataset is centered around the origin,sklearn's PCA class take care of it but data need to be centered otherwise\n",
    "* Once we know all the PCs we can now reduce the dimensions to *d* dimensions (d<PCs), projection can simply be done by matrix multiplication of **X**(original dataset) and $W_{d}$ (it is a matrix with first d PCs{each column represent one PC})\n",
    "```python\n",
    "W2 = Vt.T[:,:2]\n",
    "X2d = X_centered.dot(W2)\n",
    "```\n",
    "* **Sci-kit learn's application**\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "```\n",
    "* Explained Variance Ratio : it is the ratio that tell us how much information/variance lies on that axis , it can accessed by explaiend_variance_ratio_ after fitting data in PCA class\n",
    "```python\n",
    ">>> pca.explaiend_variance_ratio_\n",
    "array([0.84248607, 0.14631839])\n",
    "```\n",
    "this tell us that 84.2% of dataset's variance lies on $1^{st}$ PC and 14.6% on $2^{nd}$ and the remaining 1.2% on the third which we are not using\n",
    "* **Choosing Right Number of Dimensions** : Instead of choosing a arbitary number as dimension we can choose the number of dimensions that add up to a sufficiently large portion or varience (eq 95%) . unless you reduciing dimension for data visualization(you just want 2-3 dimensions)<br>\n",
    "to apply , insted or setting n_component to a number we can just set it to float between 0.0 and 1.0 which is the fraction of varience we want to cover\n",
    "```python\n",
    "pca = PCA(n_components=0.95) #for 95% of variance\n",
    "X_reduces = pca.fit_transform(X)\n",
    "```\n",
    "* **PCA for Compression**: PCA can also be used for compression and Decompression , eg. We can reduce dimension of MNIST dataset to 95% variance and we can get back the image by .inverse_transform(reduces_matrix) , ofc. the decompressed image wont be same quality as it lost 5% varience but it would be comparable\n",
    "```python\n",
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "```\n",
    "<img src='https://i.ibb.co/HNLJxgp/Screenshot-from-2022-11-30-13-12-41.png' width=45%><br>\n",
    "* **Randomized PCA** : if we set svd_solver='randomized' , SciKit-learn will use a stochastic algorithm called Randomized PCA, that is much faster than standard SVD with computational-complexity of $O(mXd^{2})+O(d^{3})$ insted of $O(mXn^{2})+O(n^{3})$\n",
    "```python\n",
    "rnd_pca = PCA(n_components=154,svd_solver='randomized')\n",
    "X_red = rnd_pca.fit_transform(X)\n",
    "```\n",
    "by default it is set to 'auto' ,ie. SciKit-learn will use randomized SVD if m or n is greater than 500\n",
    "<br><br>\n",
    "* **Incremental PCA** : Many time we don't have enough resources(RAM) to load whole data at once but we want to perform PCA , in that case we can use IncrementalPCA class with small batches of data .\n",
    "```python\n",
    "from skllearn.decomposition import IncrementalPCA\n",
    "inc_pca = IncrementalPCA(n_components=124)\n",
    "for X_batch in np.array_split(X_train,100):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_red = inc_pca.transform(X_train)\n",
    "```\n",
    "alternative approch is to use NumPy's memmap() which only load the data that is needed , we can use the .fit() as memmap only bring the needed data by the operation\n",
    "```python\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", \n",
    "                   mode=\"readonly\", shape=(m, n)) \n",
    "                   #m is n_instance and n is n_fet\n",
    "batch_size = m // 100\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "```\n",
    "our data should already be converted to the memmap file (Memory-Mapped File)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192f6ab",
   "metadata": {},
   "source": [
    "#### ***kPCA*** (Kernel PCA)\n",
    "* We know kernel trick is a mathematical technique that implicitly maps instances into very high dimensional space , hence creating it easier(possible) for classification and regression. As we used in SVM\n",
    "* Since , some data don't work fine with linear dimensionality reduction technique so we have to first increase its dimension then apply PCA for better result\n",
    "<br>\n",
    "\n",
    "**Selecting Kernel and hyperparameter tuning**\n",
    "***\n",
    "* kPCA is an unsupervised algorithm(it need no target just X on which PCA will be applied) but its generally used as a preperation step for a supervised learning task(classification and regression),hence we can use grid search on it to find kernel and tune hyperparameters\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([\n",
    "(\"kpca\", KernelPCA(n_components=2)),\n",
    "(\"log_reg\", LogisticRegression())\n",
    "])\n",
    "param_grid = [{\n",
    "\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "\"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "```\n",
    "\n",
    "* Another totally unsupervised way is to choose kernel & hyperparameter with lowest reconstruction error\n",
    "```python\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "                    fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "#### reconstructional error ####\n",
    "from sklearn.metrics import mean_squared_error\n",
    "err = mean_squared_error(X, X_preimage)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd2865",
   "metadata": {},
   "source": [
    "#### LLE (Locally Linear Embedding)\n",
    "* it is also a nonlinear dimensionality reduction algorithm but unlike other which are based on projections , LLE is based on linear relationship of instance with its neighbour instances hence preserve the geomatric shape as well\n",
    "*LLE works by first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved\n",
    "```python\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)\n",
    "```\n",
    "Steps involved in LLE :-\n",
    "* Step-1 : For each training instance $x^{(i)}$ algorithm will find its k closest neightbour , then it will find weights $w_{i,j}$(weight for $j^{th}$ neighbour of $i^{th}$ instance) such that the squared distance between $x^{(i)}$ and $Σ_{j=1}^{m}w_{(i,j)} x^{(j)}$ is minimum (W is matrix of all the weights $w_{i,j}$)\n",
    "<img src='https://i.ibb.co/71mhgdM/6.png' width=45%><br>\n",
    "\n",
    "* Step-2 : Now we will map the training instances into d-dimension(d<n) while keeping the local linear relation as intact as possible. If $z^{(i)}$ is the image of $x^{(i)}$ in the d-dimension then we have to minimise the squared distance between $z^{(i)}$ and $Σ_{j=1}^{m}w_{(i,j)} z^{(j)}$ . *it look very similar to the first step but now our goal is reversed in first step we were finding optimal weight keeping instances constant , and now we are keeping weight constant while finding best image ($z^{(i)}$) of the $x^{(i)}$ instance*\n",
    "<img src='https://i.ibb.co/nzGbTpS/7.png' width=45%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e0d51",
   "metadata": {},
   "source": [
    "#### Other Dimensionality Reduciotion Techniques\n",
    "***\n",
    "* **Mulridimensional Scaling(MDS)** : Reduce Dimension while tying to preserve the distance between the instances\n",
    "<br>\n",
    "\n",
    "* **Isomap** : It creates a graph by connecting each instance to its nearest neighbors, then\n",
    "reduces dimensionality while trying to preserve the geodesic distances9 between\n",
    "the instances.\n",
    "<br>\n",
    "\n",
    "* **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D)\n",
    "<br>\n",
    "\n",
    "* **Linear Discriminant Analysis (LDA)**: is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784096e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
