{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c229fbd",
   "metadata": {},
   "source": [
    "### ***Ensemble Learning***\n",
    "The idea is based on ***wisdom of crowd*** , ie. if we give a complex problem to 1000 random people , we will find their aggregated answer is better than an expert's answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a89c40",
   "metadata": {},
   "source": [
    "#### Voting classifiers\n",
    "* We traing multiple classifier then return prediction which is most-common/frequent/mode in the prediction made by individual classifiers , this is called *hard-voting classifier*\n",
    "* It's based on *Law of large numbers* , ie. if take 1000 of *weak* classifier with only 51% accuracy and use hard-voting we can hope for accuracy around 75% | condition : all individual classifier must be trained on different dataset as same classifier trained on same dataset will make same mistake , which is of no good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "148588ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "X,y = make_moons(n_samples=500,noise=0.4,random_state=42)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "ln_reg = LogisticRegression(random_state=42)\n",
    "svc = SVC(gamma='scale',random_state=42)\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr',ln_reg),('svc',svc),('rfc',rfc)],\n",
    "                              voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1630a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.83\n",
      "SVC : 0.83\n",
      "RandomForestClassifier : 0.81\n",
      "VotingClassifier : 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (ln_reg,svc,rfc,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,\":\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac71ab",
   "metadata": {},
   "source": [
    "*if all the classifiers used in voting-classifier are able to estimate class-probability(ie if it has predict_proba() method) then we can return class with highest average-probability in all classes , this is called soft-voting and genrelly performs better as it give weight to more confident class.*<br><br>\n",
    "All we need is to change parameter *voting* from 'hard' to 'soft'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fdec8",
   "metadata": {},
   "source": [
    "#### Bagging (bootstrap aggregating) and Pasting\n",
    "* One way to produce diverse classifiers is to use totaly different algorithms as discussed above another way is to use the same algorithm but training it with random subset of training dataset\n",
    "* When these subset were formed *with* replacement(ie. One instance can ouccur in two or more subset) is called ***bagging*** and without replacement(unique instance in all subset) called ***pasting***\n",
    "* Once all predictors are trained then we can return aggregated prediction ie. *Statistical mode*(just like hard-voting) and return average in case of regression<br><br>\n",
    "\n",
    "*BaggingClassifier() automatically uses soft-voting if provided classifier has predict_proba() method*<br>\n",
    "*generelly bagging result model with comparable bias but lower variance than the estimator it made up on*<br><br>\n",
    "\n",
    "***OOB instances*** : in bagging some instance might occured multiple time but some do not occur at all , these instance are called out-of-bag instance(OOB) . Since these instances were never seen by predictor we can use these as validation set . just set *oob_score=True* while defining bagging_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8d20cba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree accuracy : 0.9\n",
      "Bagging oob-score      : 0.925\n",
      "Bagging Classifier acc : 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X,y = make_moons(noise=0.25,n_samples=200,random_state=42)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                                n_estimators=500,max_samples=100,\n",
    "                                bootstrap=True,n_jobs=-1,oob_score=True)\n",
    "\n",
    "bagging_clf.fit(X_train,y_train)\n",
    "tree.fit(X_train,y_train)\n",
    "\n",
    "print(f'''\n",
    "Decision Tree accuracy : {accuracy_score(y_test,tree.predict(X_test))}\n",
    "Bagging oob-score      : {bagging_clf.oob_score_}\n",
    "Bagging Classifier acc : {accuracy_score(y_test,bagging_clf.predict(X_test))}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb87d9",
   "metadata": {},
   "source": [
    "##### ***Random Patches and Random Subspace***\n",
    "just like we sampled training instances(bagging/pasting) we can sample features as well by using two hyperparameter *max_features* and *bootstrap_features*<br><br>\n",
    "**Random Patches** : When we do sampling of both features and instances <br><br>\n",
    "**Random Subspace**: When we only do sampling of features by setting *bootstrap=False* , *max_sample=1* but keeping *bootstrap_feature=True* and *max_features < 1* <br>\n",
    "*sampling features produce more diverse predictor , by trading a bit more bias for a lower varience*<br><br>\n",
    "*Bias : The inability of model to truly capture the relationship in training data ie. High bias mean it has high error when tested we training data itself*<br>\n",
    "*Variance : Diffrence of fit on training and testing datasets , ie. High variance mean model have low bias/ lower training error but high testing error*<br>\n",
    "*A model with low bias but high variance is said to be over-fitting and the model with high bias is said to be underfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb0b7b",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "* These are ensemble of Decision Trees trained on bagging method\n",
    "* Random Forest introduces more randomness when growing trees , as inseted of searching for the best feature when splitting a node it searches for the best feature among the random subset of features , which again trade bias for better varience<br><br>\n",
    "\n",
    "##### Extra-Tree (Extreamly-Random-Tree)\n",
    "when we are splitting a node only one random subset is consider , we can make it more random by using random threshold for splitting rather than the best possible threshold . These extreamly random trees are called ***extra-trees***<br><br>\n",
    "\n",
    "*another great quality of random forest is that it make it easy to find the relative importance of features , in scikit-learn it is calculated by how much the impurity of node reduces as it uses that feature . scikit-learn automatically calculates it after training the forest we can acess the result using \"feature_importance_\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4908087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of RandomForest : 1.0\n",
      "\n",
      "\n",
      "FEATURE IMPORTANCE\n",
      "\n",
      "sepal length (cm) : 0.10488337961069605\n",
      "sepal width (cm) : 0.02538391252457476\n",
      "petal length (cm) : 0.41879788130654844\n",
      "petal width (cm) : 0.4509348265581809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X ,y = iris.data , iris.target\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "print(f\"accuracy of RandomForest : {accuracy_score(y_test,rfc.predict(X_test))}\\n\\n\")\n",
    "print(\"FEATURE IMPORTANCE\\n\")\n",
    "for feat_name,importance in zip(iris.feature_names,rfc.feature_importances_):\n",
    "    print(feat_name,\":\",importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575e98f",
   "metadata": {},
   "source": [
    "#### ***Boosting***\n",
    "The generel idea is to combine several waek learners to strong learner , which is mainly done by training models sequentially , each trying to corrects its predecessor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b059fed",
   "metadata": {},
   "source": [
    "#### AdaBoost (Adaptive Boosting)\n",
    "* The idea is to give more importance to the misclassified/underfitted instances\n",
    "* In this , first model is trained on whole training dataset then the weight of missclassifed instances get increase then second model trained on updated weights then it again update weights and so on.\n",
    "* After all predictors are trained , AdaBoost predicts just like bagging or pasting except the predictors have different weight as per their accuracy on the weighted dataset<br>\n",
    "<img src='https://i.ibb.co/HGD905K/Screenshot-from-2022-11-23-16-15-04.png' width='60%'><br>\n",
    "\n",
    "*above image shows how new predictor is better then the last one and also shows the affect of learning rate similar that of gradient decent , but insted of tweaking hyperparameter to minimise cost-function AdaBoost add new predictor to the ensemble with higher weight*<br>\n",
    "*the model used in image is highly regulerized/underfitting SVM that's why had high bias*<br><br>\n",
    "*For maths involved in finding and updating weight check notebook*<br><br>\n",
    "scikit-learn actually uses multiclass version of AdaBoost called ***SAMME*** stands for *Stagewise Additive Modeling using Multiclass Exponential loss-function* , moreover if the predictor/base_estimator can return class probability scikit-learn uses varient of SAMME called SAMME.R (R = Real) which relies on class probability rather than prediction ,hence perform better<br><br>\n",
    "*decision_tree with depth=1 is called decision_stump*<br>\n",
    "*to avoid over-fitting we can reduce n_estimator or use a more regularized base_estimator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "879df7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost accuracy : 96.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                            learning_rate=0.5,n_estimators=200,algorithm='SAMME.R')\n",
    "\n",
    "ada_clf.fit(X_train,y_train)\n",
    "print(f'AdaBoost accuracy : {accuracy_score(y_test,ada_clf.predict(X_test))*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669772f",
   "metadata": {},
   "source": [
    "#### Gradient Boosting \n",
    "Gradient boost just like AdaBoot based on sequentially adding predictor to the ensemble , but insted of tweaking instance's weight(like AdaBoost) it tries to fit new predictor to the residual error made by previous predictor<br><br>\n",
    "\n",
    "Gradient boost with DecisionTreeRegressor as a base estimator is called *Gradient Tree Boosting* or *Gradient Boosted Regression Tree* ***(GBRT)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff4dcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_tree_pred(tree,clr,label,axes=[-0.5,0.5,-0.1,0.8]):\n",
    "    x_new = np.linspace(axes[0],axes[1],100).reshape(-1,1)\n",
    "    y_new = tree.predict(x_new)\n",
    "    plt.plot(x_new,y_new,clr+'-',label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b74cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of ensembled GBRT is 0.10707621035225663\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#### Dummy Data ####\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "np.random.seed(24)\n",
    "X_test =  np.random.rand(100, 1) - 0.5\n",
    "y_test = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "#### creating a GBRT ####\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X,y3)\n",
    "\n",
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1,tree_reg2,tree_reg3))\n",
    "print(f'MSE of ensembled GBRT is {mean_squared_error(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723cbec2",
   "metadata": {},
   "source": [
    "we can get similar result with sklearn's GradientBoostRegressor Class , and just like RandomForestRegressor we can control the growth of DecisionTree by *max_sample_leafs , max_depth* etc and number of trees with *n_estimators*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "588e4b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 450x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)#making similar model as above\n",
    "gbrt.fit(X,y)\n",
    "\n",
    "plt.figure(figsize=(4.5,3.5))\n",
    "plt.plot(X,y,'b.')\n",
    "plt_tree_pred(gbrt,'g','GBRT')\n",
    "plt_tree_pred(tree_reg1,'r','DecisionTree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209cdbb",
   "metadata": {},
   "source": [
    "The *learning_rate* hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called ***shrinkage***<br><br>\n",
    "\n",
    "hence,we can use early stopping to find optimal number of trees.<br>\n",
    "* one way is to use *staged_predict()* method which return an iterator of prediction made at each step/predictor by GradientBoostingRegressor (its just a list of predictor made at each step)\n",
    "* Another way , is to not train ensemble on whole large number of trees and actually early_stopping , this could be done by keeping *warm_start=True* which make sklearn to keep existing tree when the *.fit()* is called and allowing increamental learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa1b69ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after early stopping with staged_predict() 0.002254233050012351\n",
      "Error after actual early stopping  0.002286453873068945\n"
     ]
    }
   ],
   "source": [
    "#### early-stopping with staged_predict() ####\n",
    "X_train , X_val , y_train , y_val = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=150)\n",
    "gbrt.fit(X_train,y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val,y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimator = np.argmin(errors)\n",
    "\n",
    "best_gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimator)\n",
    "best_gbrt.fit(X_train,y_train)\n",
    "\n",
    "print(\"Error after early stopping with staged_predict()\",\n",
    "      mean_squared_error(y_val,best_gbrt.predict(X_val)))\n",
    "\n",
    "#### Actually Early-Stopping ####\n",
    "gbrt = gbrt = GradientBoostingRegressor(max_depth=2,warm_start=True)\n",
    "min_error = float('inf')\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1,151):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train,y_train)\n",
    "    val_error = mean_squared_error(y_val,gbrt.predict(X_val))\n",
    "    \n",
    "    if val_error < min_error :\n",
    "        min_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        if error_going_up == 5:\n",
    "            break\n",
    "print(\"Error after actual early stopping \",\n",
    "      mean_squared_error(y_val,gbrt.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b70d69",
   "metadata": {},
   "source": [
    "both method end up with almost same result but 2nd method will be faster as it don't have to train all large number of trees <br><br>\n",
    "\n",
    "GradientBoostingRegressor also support **subsample** hyperparameter which specifier the fraction of training set can be used to train a tree , if *subsample=0.25* it will train trees with 25% of data and this data is selected randomly and yes it trade a little bias for better variance its called Stocastic Gradient Boosting  <br><br>\n",
    "\n",
    "***XGBoost*** : its an optimized version of GradientBoost and it can itself take care of early stopping ,its  genreally perform better.\n",
    "\n",
    "    import xgboost\n",
    "    xgb_reg = xgboost.XGBRegressor()\n",
    "    xgb_reg.fit(X_train, y_train,eval=[(X_val,y_val)],early_stopping_rounds=2)\n",
    "    y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a04ba",
   "metadata": {},
   "source": [
    "#### Stacking (stacked generalization)\n",
    "the idea is ,insted of using trivial function to aggregate the prediction made by predictor we can use another model to aggregate the predictions , by training it with predictor's predictions and input and final result as target.\n",
    "<br><br>\n",
    "if we want to create a 2 layer stacking we have to split data in 2 subset , one will be used to train the base_estimator , and the other to get the prediction from base_estimators and use it to train blender(which itself a model) ,that will genrate final result\n",
    "<br><br>\n",
    "*sklearn don't have direct implimentation for stacking but its not too do our own implimentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea450103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
